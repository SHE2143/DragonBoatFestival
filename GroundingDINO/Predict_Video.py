from groundingdino.util.inference import load_model, predict, annotate
from transformers import AutoTokenizer, AutoModel
import cv2
import os

# ç¼“å­˜ BERT æ¨¡å‹
AutoTokenizer.from_pretrained("bert-base-uncased")
AutoModel.from_pretrained("bert-base-uncased")

# åŠ è½½ GroundingDINO æ¨¡å‹
model = load_model(
    "/root/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py",
    "/root/GroundingDINO/weights/groundingdino_swint_ogc.pth"
)

# åŠ è½½ GroundingDINO æ¨¡å‹
# model = load_model(
#     "/root/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py",
#     "/root/GroundingDINO/weights/groundingdino_swinb_cogcoor.pth"
# )

# æç¤ºè¯ä¸é˜ˆå€¼
TEXT_PROMPT = " people . bicycle . car. van . truck . tricycle . awning-tricycle . bus . motor"
BOX_THRESHOLD = 0.5
TEXT_THRESHOLD = 0.5

# ç±»åˆ«æ˜ å°„
PHRASE_TO_CLASSID = {
    "pedestrain": 1,
    "people": 2,
    "bicycle": 3,
    "car": 4,
    "van": 5,
    "truck": 6,
    "tricycle": 7,
    "awning tricycle": 8,
    "bus": 9,
    "motor": 10,
}

# è¾“å…¥è§†é¢‘è·¯å¾„
VIDEO_PATH = "/Path/to/video"

# è¾“å‡ºç›®å½•
OUTPUT_IMAGE_DIR = "/root/GroundingDINO/output_video/images"
OUTPUT_LABEL_DIR = "/root/GroundingDINO/output_video/labels"
OUTPUT_VIDEO_PATH = "/root/GroundingDINO/output_video/detection_result.mp4"
os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)
os.makedirs(OUTPUT_LABEL_DIR, exist_ok=True)

# æ‰“å¼€è§†é¢‘
cap = cv2.VideoCapture(VIDEO_PATH)
frame_index = 0

# è·å–è§†é¢‘å‚æ•°ï¼ˆç”¨äºè¾“å‡ºï¼‰
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# åˆ›å»ºè§†é¢‘å†™å…¥å™¨
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # å¯æ”¹ä¸º 'XVID'ã€'avc1' ç­‰
out_video = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break  # è§†é¢‘ç»“æŸ

    print(f"Processing frame {frame_index}...")

    h, w = frame.shape[:2]
    image_source = frame.copy()
    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # æ¨ç†
    boxes, logits, phrases = predict(
        model=model,
        image=image_rgb,
        caption=TEXT_PROMPT,
        box_threshold=BOX_THRESHOLD,
        text_threshold=TEXT_THRESHOLD
    )

    # ä¿å­˜å›¾åƒå¸§
    image_name = f"frame_{frame_index:06d}.jpg"
    out_image_path = os.path.join(OUTPUT_IMAGE_DIR, image_name)
    annotated = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)
    cv2.imwrite(out_image_path, annotated)

    # å†™å…¥è§†é¢‘å¸§
    out_video.write(annotated)

    # ä¿å­˜æ ‡ç­¾
    out_label_path = os.path.join(OUTPUT_LABEL_DIR, f"frame_{frame_index:06d}.txt")
    with open(out_label_path, "w") as f:
        for box, phrase, logit in zip(boxes, phrases, logits):
            phrase = phrase.lower()
            class_id = PHRASE_TO_CLASSID.get(phrase, -1)
            if class_id == -1:
                continue

            x1, y1, x2, y2 = box.tolist()
            conf = float(logit)
            f.write(f"{class_id} {x1:.6f} {y1:.6f} {x2:.6f} {y2:.6f} {conf:.6f}\n")

    frame_index += 1

cap.release()
out_video.release()
print("âœ… è§†é¢‘å¤„ç†å®Œæ¯•ï¼Œæ£€æµ‹ç»“æœä¿å­˜åœ¨ï¼š")
print(f"ğŸ“ å›¾åƒå¸§ï¼š{OUTPUT_IMAGE_DIR}")
print(f"ğŸ“ æ ‡ç­¾ï¼š{OUTPUT_LABEL_DIR}")
print(f"ğŸ¥ åˆæˆè§†é¢‘ï¼š{OUTPUT_VIDEO_PATH}")
