import os
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
from sahi.utils.cv import visualize_object_predictions
from PIL import Image
import numpy as np

# ==== æ¨¡å‹åˆå§‹åŒ– ====
detection_model = AutoDetectionModel.from_pretrained(
    model_type="ultralytics",
    model_path="/Path/to/weight",   # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    confidence_threshold=0.25,
    device="0",  # "cpu" æˆ– "0"
)

# ==== è¾“å…¥è¾“å‡ºè·¯å¾„é…ç½® ====
image_dir = "/Path/to/imgdir"  # æ›¿æ¢ä¸ºå›¾ç‰‡è·¯å¾„
output_dir = "output_sahi_video"
output_image_dir = os.path.join(output_dir, "images")
output_label_dir = os.path.join(output_dir, "labels")
os.makedirs(output_image_dir, exist_ok=True)
os.makedirs(output_label_dir, exist_ok=True)

# ==== æ»‘çª—å‚æ•° ====
SLICE_HEIGHT = 256
SLICE_WIDTH = 256
OVERLAP_RATIO = 0.2

# ==== éå†å›¾ç‰‡ ====
for img_name in os.listdir(image_dir):
    if not img_name.lower().endswith((".jpg", ".jpeg", ".png")):
        continue

    img_path = os.path.join(image_dir, img_name)
    image = Image.open(img_path).convert("RGB")
    w, h = image.size

    # ==== SAHIæ»‘çª—æ¨ç† ====
    result = get_sliced_prediction(
        image,
        detection_model,
        slice_height=SLICE_HEIGHT,
        slice_width=SLICE_WIDTH,
        overlap_height_ratio=OVERLAP_RATIO,
        overlap_width_ratio=OVERLAP_RATIO,
    )

    # ==== ç”ŸæˆYOLOæ ‡ç­¾ ====
    yolo_lines = []
    for det in result.object_prediction_list:
        x1, y1, x2, y2 = det.bbox.to_xyxy()
        conf = det.score.value
        class_id = det.category.id  # å¦‚æœä½ è®­ç»ƒæ˜¯ä»0å¼€å§‹ï¼Œè¿™é‡Œå»æ‰+1

        x_center = ((x1 + x2) / 2) / w
        y_center = ((y1 + y2) / 2) / h
        width = (x2 - x1) / w
        height = (y2 - y1) / h

        yolo_lines.append(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f} {conf:.6f}")

    # ==== ä¿å­˜txt ====
    txt_name = os.path.splitext(img_name)[0] + ".txt"
    txt_path = os.path.join(output_label_dir, txt_name)
    with open(txt_path, "w") as f:
        f.write("\n".join(yolo_lines))

    # ==== ä¿å­˜å¯è§†åŒ–å›¾åƒ ====
    vis_img = visualize_object_predictions(
        image=np.array(image),
        object_prediction_list=result.object_prediction_list,
    )
    out_img_path = os.path.join(output_image_dir, img_name)
    Image.fromarray(vis_img["image"]).save(out_img_path)

print("âœ… SAHI æ»‘çª—æ£€æµ‹å®Œæˆï¼è¾“å‡ºç»“æœå·²ä¿å­˜ï¼š")
print(f"ğŸ“ æ ‡ç­¾è·¯å¾„ï¼š{output_label_dir}")
print(f"ğŸ“ å›¾åƒè·¯å¾„ï¼š{output_image_dir}")
